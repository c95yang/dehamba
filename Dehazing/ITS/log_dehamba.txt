Namespace(model_name='DeHambaNet', mode='train', data_dir='/home/cc/Documents/data/reside-indoor', batch_size=4, learning_rate=0.0001, weight_decay=0, num_epoch=300, print_freq=100, num_worker=8, save_freq=10, valid_freq=10, resume='', gamma=0.5, test_model=None, save_image=False, model_save_dir='results/mean/full', result_dir='results/DeHambaNet/test')
Warning: module PatchEmbed is treated as a zero-op.
Warning: module PatchUnEmbed is treated as a zero-op.
Warning: module Dropout is treated as a zero-op.
Warning: module SiLU is treated as a zero-op.
Warning: module SS2D is treated as a zero-op.
Warning: module DropPath is treated as a zero-op.
Warning: module Sigmoid is treated as a zero-op.
Warning: module ChannelAttention is treated as a zero-op.
Warning: module CAB is treated as a zero-op.
Warning: module VSSBlock is treated as a zero-op.
Warning: module BasicLayer is treated as a zero-op.
Warning: module ResidualGroup is treated as a zero-op.
Warning: module DeHambaNet is treated as a zero-op.
DeHambaNet(
  3.16 M, 73.355% Params, 12.85 GMac, 99.610% MACs, 
  (conv_first): Conv2d(2.69 k, 0.062% Params, 11.01 MMac, 0.085% MACs, 3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    192, 0.004% Params, 393.22 KMac, 0.003% MACs, 
    (norm): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
  (pos_drop): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)
  (layers): ModuleList(
    (0): ResidualGroup(
      767.54 k, 17.824% Params, 3.12 GMac, 24.200% MACs, 
      (residual_group): BasicLayer(
        684.5 k, 15.895% Params, 2.78 GMac, 21.562% MACs, dim=96, input_resolution=(64, 64), depth=6
        (blocks): ModuleList(
          (0): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.000)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (1): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.004)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (2): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.009)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (3): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.013)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (4): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.017)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (5): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.022)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (conv): Conv2d(83.04 k, 1.928% Params, 340.13 MMac, 2.637% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (patch_unembed): PatchUnEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    )
    (1): ResidualGroup(
      767.54 k, 17.824% Params, 3.12 GMac, 24.200% MACs, 
      (residual_group): BasicLayer(
        684.5 k, 15.895% Params, 2.78 GMac, 21.562% MACs, dim=96, input_resolution=(64, 64), depth=6
        (blocks): ModuleList(
          (0): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.026)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (1): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.030)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (2): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.035)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (3): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.039)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (4): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.043)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (5): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.048)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (conv): Conv2d(83.04 k, 1.928% Params, 340.13 MMac, 2.637% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (patch_unembed): PatchUnEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    )
    (2): ResidualGroup(
      767.54 k, 17.824% Params, 3.12 GMac, 24.200% MACs, 
      (residual_group): BasicLayer(
        684.5 k, 15.895% Params, 2.78 GMac, 21.562% MACs, dim=96, input_resolution=(64, 64), depth=6
        (blocks): ModuleList(
          (0): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.052)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (1): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.057)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (2): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.061)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (3): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.065)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (4): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.070)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (5): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.074)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (conv): Conv2d(83.04 k, 1.928% Params, 340.13 MMac, 2.637% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (patch_unembed): PatchUnEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    )
    (3): ResidualGroup(
      767.54 k, 17.824% Params, 3.12 GMac, 24.200% MACs, 
      (residual_group): BasicLayer(
        684.5 k, 15.895% Params, 2.78 GMac, 21.562% MACs, dim=96, input_resolution=(64, 64), depth=6
        (blocks): ModuleList(
          (0): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.078)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (1): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.083)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (2): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.087)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (3): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.091)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (4): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.096)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
          (5): VSSBlock(
            114.08 k, 2.649% Params, 463.47 MMac, 3.594% MACs, 
            (ln_1): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
            (self_attention): SS2D(
              57.6 k, 1.338% Params, 235.14 MMac, 1.823% MACs, 
              (in_proj): Linear(36.86 k, 0.856% Params, 150.99 MMac, 1.171% MACs, in_features=96, out_features=384, bias=False)
              (conv2d): Conv2d(1.92 k, 0.045% Params, 7.86 MMac, 0.061% MACs, 192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): SiLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
              (out_norm): LayerNorm(384, 0.009% Params, 786.43 KMac, 0.006% MACs, (192,), eps=1e-05, elementwise_affine=True)
              (out_proj): Linear(18.43 k, 0.428% Params, 75.5 MMac, 0.585% MACs, in_features=192, out_features=96, bias=False)
            )
            (drop_path): DropPath(0, 0.000% Params, 0.0 Mac, 0.000% MACs, drop_prob=0.100)
            (conv_blk): CAB(
              56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
              (cab): Sequential(
                56.1 k, 1.303% Params, 227.54 MMac, 1.764% MACs, 
                (0): Conv2d(27.68 k, 0.643% Params, 113.38 MMac, 0.879% MACs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): GELU(0, 0.000% Params, 131.07 KMac, 0.001% MACs, approximate='none')
                (2): Conv2d(27.74 k, 0.644% Params, 113.64 MMac, 0.881% MACs, 32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ChannelAttention(
                  675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                  (attention): Sequential(
                    675, 0.016% Params, 393.89 KMac, 0.003% MACs, 
                    (0): AdaptiveAvgPool2d(0, 0.000% Params, 393.22 KMac, 0.003% MACs, output_size=1)
                    (1): Conv2d(291, 0.007% Params, 291.0 Mac, 0.000% MACs, 96, 3, kernel_size=(1, 1), stride=(1, 1))
                    (2): ReLU(0, 0.000% Params, 3.0 Mac, 0.000% MACs, inplace=True)
                    (3): Conv2d(384, 0.009% Params, 384.0 Mac, 0.000% MACs, 3, 96, kernel_size=(1, 1), stride=(1, 1))
                    (4): Sigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
                  )
                )
              )
            )
            (ln_2): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (conv): Conv2d(83.04 k, 1.928% Params, 340.13 MMac, 2.637% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
      (patch_unembed): PatchUnEmbed(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )
    )
  )
  (norm): LayerNorm(192, 0.004% Params, 393.22 KMac, 0.003% MACs, (96,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(83.04 k, 1.928% Params, 340.13 MMac, 2.637% MACs, 96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_last): Conv2d(2.6 k, 0.060% Params, 10.63 MMac, 0.082% MACs, 96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
Model FLOPs: 12.9 GMac
Model Parameters: 4.31 M
/home/cc/Documents/FocalNet/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Time:  1.2182 Epoch: 001 Iter:  100/3498 LR: 0.0000333333 Loss content:  0.1387 Loss fft:  0.8029
Time:  1.2182 Epoch: 001 Iter:  200/3498 LR: 0.0000333333 Loss content:  0.1171 Loss fft:  0.6153
